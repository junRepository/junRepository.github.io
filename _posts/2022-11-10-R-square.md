---
layout: post
title:  결정계수(R-square)와 과대적합, 과소적합 
date:   2022-11-10 20:00
image:  
tags:   machine_learning
---

# R-square(결정계수,R^2)  

K-NN은 데이터를 분류 하는 것이다, 1아니면 0인지 예측한 후 분류를 하여 정확하게 분류한 개수의 비율을 정확도라고 한다.  
반면에  regression(회귀)는 정확한 숫자를 맞힌다는 것은 어렵다. 왜냐하면 예측 값이나 target 모두 임의의 수치이기 때문이다.  
그래서 K-NN regression의 정확도를 __결정계수__ 라고 부른다.  
* 결정계수를 구하는 식은 R^2 = 1 - [(타겟 - 에측)^2 / (타깃 - 평균)^2]이다.
* 결정계수가 0이면 타깃의 평균 정도 예측을 하는 것이고, 1에 가까워지면 타깃이 예측에 아주 가까워진다는 것이다.  
##### 결정계수 코드
```py
from sklearn.metrics import mean_absolute_error
#test에 대한 예측을 한다.
test_prediction = knr.predict(test_input)
#test에 대한 평균 절댓값 오차를 계산한다.
mae = mean_absolute_error(test_target, test_prediction)
```

# 과대적합(overfitting), 과소적합(underfitting)
1. 과대적합: train의 점수 > test의 점수
    * 해결법: 모델을 덜 복잡하게 만들어야 한다. => k값을 늘린다.
2. 과소적합: train의 점수 < test의 점수  or  train의 점수와 test의 점수가 둘 다 낮은 것(모델이 단순해서 train에서 적절히 훈련이 안 됐다.)
    * 해결접: k값을 줄이거나(k-nn모델), 모델을 복잡하게 만들어야 한다.

